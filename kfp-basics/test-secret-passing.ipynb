{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import itertools\n",
    "from kubernetes import client as k8s_client\n",
    "from kubernetes.client.models import V1SecretKeySelector, V1EnvVar, V1Secret, V1SecretEnvSource\n",
    "import time\n",
    "\n",
    "# Name of our experiment in kubeflow\n",
    "EXPERIMENT_NAME = \"secret-passing-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetLookaroundOp(dsl.ContainerOp):\n",
    "    \"\"\"\n",
    "    Generator for a LookaroundOp. \n",
    "    \n",
    "    The LookaroundOp runs a script that looks around, printing the environment vars and tree\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            image=f\"k8scc01covidacr.azurecr.io/secret-passing-test:v5\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=EXPERIMENT_NAME,\n",
    "    description=\"Test of passing secrets into pipelines\",\n",
    ")\n",
    "def secret_passing_test_pipeline():\n",
    "    \n",
    "    lookaround = GetLookaroundOp(name=\"Lookaround1\")\n",
    "\n",
    "    lookaround2 = GetLookaroundOp(name=\"LookaroundWithEnvVar\")\n",
    "    lookaround2 = lookaround2.add_env_variable(\n",
    "        k8s_client.V1EnvVar(\n",
    "            name=\"ENV_VAR_TEST_1\",\n",
    "            value=\"ENV_VAR_TEST_1_VALUE\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    lookaround3 = GetLookaroundOp(name=\"LookaroundWithSecretEnvVar\")\n",
    "    lookaround3 = lookaround3.add_env_variable(\n",
    "        k8s_client.V1EnvVar(\n",
    "            name=\"ENV_VAR_TEST_SECRET_1\",  # The env var will end up with this name\n",
    "            value_from=k8s_client.V1EnvVarSource(\n",
    "                # This is a secret KEY selector.  It selects a key of a secret, eg\n",
    "                # one piece of a secret.  See https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-using-kubectl\n",
    "                # for better explanation of kubeflow secrets\n",
    "                secret_key_ref=k8s_client.V1SecretKeySelector(\n",
    "                    name=\"db-user-pass\",  # Name of the secret we want to pull a value from  (this is a secret in the pod's namespace?)\n",
    "                    key=\"username.txt\"   # The secret.key we want to pull.  This names the part of the secret we pull from\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_yaml_zip = EXPERIMENT_NAME + '.zip'\n",
    "kfp.compiler.Compiler().compile(secret_passing_test_pipeline,\n",
    "                                experiment_yaml_zip,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/9a83ae66-e0ca-479b-8280-93d540fe9d12\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/341fc6c5-e523-440a-814b-915cf8ad0ed9\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(\n",
    "    exp.id,\n",
    "    EXPERIMENT_NAME + '-' + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    EXPERIMENT_NAME + '.zip',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "This notebook documents:\n",
    "* How to define a simple pipeline defined by operations that are contained in docker containers\n",
    "* An example of a pipeline where each step communicates its outputs to the next step by writing them to a file locally and having kubeflow pipelines transfer results between steps as JSON strings\n",
    "* A map-reduce workflow pattern, where we:\n",
    "    * break our work into many small pieces that can be done in parallel (map), and then\n",
    "    * aggregate the product of that work back to some final result (reduce)\n",
    "\n",
    "To bring it all together, we apply these techniques to compute an estimate of pi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to estimate pi, in the most ridiculously parallel way possible\n",
    "\n",
    "This pipeline estimates pi by repeating the process of:\n",
    "\n",
    "* Picking a random location inside a 2x2 square centered on the origin\n",
    "* Checking whether or not that point also resides inside a unit circle centered on the origin\n",
    "* Assigning a value to this point:\n",
    "    * value = 1 if the point is inside the circle (red)\n",
    "    * value = 0 if the point is outside the circle (blue)\n",
    "\n",
    "By doing this repeatedly and taking 4x the average value over all repetitions, we obtain an estimate of pi\n",
    "\n",
    "![Parallel Monte Carlo](images/Pi.png)\n",
    "\n",
    "We implement this procedure using the map-reduce pattern by:\n",
    "* **Map:** Generating N **sample** operations which pick the point and assign it a value of 0/1.  Note that each **sample** operation is given a different random seed to ensure it picks a different point in the square\n",
    "* **Reduce:** Combining all **sample** results in an **average** step which then returns the estimate of pi\n",
    "\n",
    "The pipeline, as visualized in kubeflow pipelines, looks like this:\n",
    "\n",
    "![The pipeline](images/kf-pipeline.png)\n",
    "\n",
    "Where the top row of **sample** operations all feed to the single **average** step on the second row.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up our Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from utilities import validate_kfp_name, validate_bucket_name\n",
    "\n",
    "#################################\n",
    "### Configure your variables ####\n",
    "#################################\n",
    "\n",
    "# Number of parallel sample steps we run\n",
    "SAMPLES = 3\n",
    "\n",
    "# Name of our experiment in kubeflow\n",
    "# Experiment name can contain alphanumeric characters, hyphens, or underscores\n",
    "EXPERIMENT_NAME = \"compute-pi-with-minio\"\n",
    "assert validate_kfp_name(EXPERIMENT_NAME)\n",
    "\n",
    "# Names and container images for our pipeline operations\n",
    "# Pipeline operations have the same name restrictions as experiments\n",
    "SAMPLE_IMAGE_PATH = f\"k8scc01covidacr.azurecr.io/blair-kf-pipeline-pi-sample:v15\"\n",
    "SAMPLE_PIPELINE_OP_NAME = \"one-pi-estimate\"\n",
    "assert validate_kfp_name(SAMPLE_PIPELINE_OP_NAME)\n",
    "\n",
    "AVERAGE_IMAGE_PATH = f\"k8scc01covidacr.azurecr.io/blair-kf-pipeline-pi-average:v16\"\n",
    "AVERAGE_PIPELINE_OP_NAME = \"aggregate-pi-estimate\"\n",
    "assert validate_kfp_name(AVERAGE_PIPELINE_OP_NAME)\n",
    "\n",
    "########################################\n",
    "### This gets fed into the map step ####\n",
    "########################################\n",
    "# Define a generator that creates the numeric random seed for each sample step\n",
    "def seeds(how_many=SAMPLES):\n",
    "    \"\"\" Define the seeds for the algorithms \"\"\"\n",
    "    for i in range(how_many):\n",
    "        yield { \"seed\" : 3 * i }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we define all operations in our pipeline, as well as how they chain together.  Pipelines are defined by separate, typically single purpose, operations (or steps).  Each pipeline operation could be used once, multiple times, etc., and might depend on results from upstream steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline here has two steps, both of which are defined in docker containers (paths to those containers were specified above and are used below).  Each step is a factory function that returns ContainerOp's.  These ContainerOps are then used to define the actual pipeline next.\n",
    "\n",
    "For this example, the containers are already built and pushed to ```k8scc01covidacr.azurecr.io``` (see ```SAMPLE_IMAGE_PATH``` and ```AVERAGE_IMAGE_PATH``` above).  Each container has a small shell script to do the work for that operation (check out the scripts at ```./sample/sample.sh``` and ```./average/average.sh``` to see how they work).  This notebook defines two kubeflow pipeline operation (```sample_op``` and ```average_op```) that specify how kubeflow interacts with those containers (how to call them, what args to provide, what to do with their outputs, ...).  \n",
    "\n",
    "Side note: Technically ```sample_op``` and ```average_op``` are factories that return ContainerOp instances, and kubeflow pipelines uses those ContainerOp instances to build a pipeline, but if none of that makes sense its ok..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "import itertools\n",
    "\n",
    "def sample_op(params):\n",
    "    \"\"\"\n",
    "    Factory for \"sample\" pipeline operation\n",
    "    \n",
    "    Operations created by this factory invoke the SAMPLE step by invoking\n",
    "    a docker container that includes ./sample/sample.sh.  sample.sh accepts\n",
    "    a random seed.\n",
    "    \n",
    "    The result of this operation will be an out.json file with contents:\n",
    "        { \"x\" : point_x_coord, \"y\" : point_y_coord, \"result\" : 0_or_1 }\n",
    "    This result will be passed back as _sample_op_result.output\n",
    "    \n",
    "    Args:\n",
    "        params (str): JSON string of a dict that has a seed value as key \"seed\", eg:\n",
    "                        '{\"seed\": 5, \"some_other_key\", \"value_doesnt_matter\"}\n",
    "\n",
    "    Returns:\n",
    "        JSON string of out.json\n",
    "    \"\"\"\n",
    "    # Assemble arguments passed to the script called by the container op\n",
    "    arguments = [\"--params\", params]\n",
    "    \n",
    "    # And to the ContainerOp constructor\n",
    "    containerop_kwargs = dict(\n",
    "        name=SAMPLE_PIPELINE_OP_NAME,\n",
    "        image=f'{SAMPLE_IMAGE_PATH}',\n",
    "        arguments=arguments,\n",
    "        # Specify where to get output from\n",
    "        file_outputs={'data': \"./output/out.json\"},\n",
    "    )\n",
    "    \n",
    "    # Return the actual Container Op, with additional memory and cpu constraints\n",
    "    return dsl.ContainerOp(\n",
    "        **containerop_kwargs,\n",
    "    ).set_memory_request(\n",
    "        \"100M\"\n",
    "    ).set_memory_limit(\n",
    "        \"150M\"\n",
    "    ).set_cpu_request(\n",
    "        \"0.1\"\n",
    "    ).set_cpu_limit(\n",
    "        \"1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client as k8s_client\n",
    "from kubernetes.client.models import V1SecretKeySelector, V1EnvVar, V1Secret, V1SecretEnvSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sample_op = sample_op(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sample_op = _sample_op.add_env_variable(\n",
    "    k8s_client.V1EnvVar(\n",
    "        name=\"ENV_TEST_1\",\n",
    "        value=\"ENV_TEST_1_VALUE\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_name = \"secret_name\"\n",
    "secret_key = \"secret_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sample_op = _sample_op.add_env_variable(\n",
    "    k8s_client.V1EnvVar(\n",
    "        name=\"ENV_SECRET_1\",  # The env var will end up with this name\n",
    "        value_from=k8s_client.V1EnvVarSource(\n",
    "            # This is a secret KEY selector.  It selects a key of a secret, eg\n",
    "            # one piece of a secret.  See https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-using-kubectl\n",
    "            # for better explanation of kubeflow secrets\n",
    "            secret_key_ref=k8s_client.V1SecretKeySelector(\n",
    "                name=secret_name,  # Name of the secret we want to pull a value from  (this is a secret in the pod's namespace?)\n",
    "                key=secret_key   # The secret.key we want to pull.  This names the part of the secret we pull from\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ContainerOp': {'is_exit_handler': False, 'human_name': 'one-pi-estimate', 'display_name': None, 'name': 'one-pi-estimate 800007fc24f3a119', 'node_selector': {}, 'volumes': [], 'tolerations': [], 'affinity': {}, 'pod_annotations': {}, 'pod_labels': {}, 'num_retries': 0, 'timeout': 0, 'init_containers': [], 'sidecars': [], 'loop_args': None, '_inputs': [], 'dependent_names': [], 'attrs_with_pipelineparams': ['node_selector', 'volumes', 'pod_annotations', 'pod_labels', 'num_retries', 'init_containers', 'sidecars', 'tolerations', '_container', 'artifact_arguments'], '_container': {'args': ['--params', '1'],\n",
      " 'command': None,\n",
      " 'env': [{'name': 'ENV_TEST_1',\n",
      "          'value': 'ENV_TEST_1_VALUE',\n",
      "          'value_from': None},\n",
      "         {'name': 'ENV_TEST_1',\n",
      "          'value': 'ENV_TEST_1_VALUE',\n",
      "          'value_from': None},\n",
      "         {'name': 'ENV_SECRET_1',\n",
      "          'value': None,\n",
      "          'value_from': {'config_map_key_ref': None,\n",
      "                         'field_ref': None,\n",
      "                         'resource_field_ref': None,\n",
      "                         'secret_key_ref': {'key': 'secret1value',\n",
      "                                            'name': 'secret1',\n",
      "                                            'optional': None}}},\n",
      "         {'name': 'ENV_SECRET_1',\n",
      "          'value': None,\n",
      "          'value_from': {'config_map_key_ref': None,\n",
      "                         'field_ref': None,\n",
      "                         'resource_field_ref': None,\n",
      "                         'secret_key_ref': {'key': 'secret_key',\n",
      "                                            'name': 'secret_name',\n",
      "                                            'optional': None}}}],\n",
      " 'env_from': None,\n",
      " 'image': 'k8scc01covidacr.azurecr.io/blair-kf-pipeline-pi-sample:v15',\n",
      " 'image_pull_policy': None,\n",
      " 'lifecycle': None,\n",
      " 'liveness_probe': None,\n",
      " 'ports': None,\n",
      " 'readiness_probe': None,\n",
      " 'resources': {'limits': {'cpu': '1', 'memory': '150M'},\n",
      "               'requests': {'cpu': '0.1', 'memory': '100M'}},\n",
      " 'security_context': None,\n",
      " 'stdin': None,\n",
      " 'stdin_once': None,\n",
      " 'termination_message_path': None,\n",
      " 'termination_message_policy': None,\n",
      " 'tty': None,\n",
      " 'volume_devices': None,\n",
      " 'volume_mounts': None,\n",
      " 'working_dir': None}, 'add_env_from': <function deprecation_warning.<locals>._wrapped at 0x7fc24f426170>, 'add_env_variable': <function deprecation_warning.<locals>._wrapped at 0x7fc24f4260e0>, 'add_port': <function deprecation_warning.<locals>._wrapped at 0x7fc24f426560>, 'add_resource_limit': <function deprecation_warning.<locals>._wrapped at 0x7fc24f426680>, 'add_resource_request': <function deprecation_warning.<locals>._wrapped at 0x7fc24f4267a0>, 'add_volume_devices': <function deprecation_warning.<locals>._wrapped at 0x7fc24f4268c0>, 'add_volume_mount': <function deprecation_warning.<locals>._wrapped at 0x7fc24f4269e0>, 'set_cpu_limit': <function deprecation_warning.<locals>._wrapped at 0x7fc24f426b00>, 'set_cpu_request': <function deprecation_warning.<locals>._wrapped at 0x7fc24f426c20>, 'set_gpu_limit': <function deprecation_warning.<locals>._wrapped at 0x7fc24f426d40>, 'set_image_pull_policy': <function deprecation_warning.<locals>._wrapped at 0x7fc24f426e60>, 'set_lifecycle': <function deprecation_warning.<locals>._wrapped at 0x7fc24f426f80>, 'set_liveness_probe': <function deprecation_warning.<locals>._wrapped at 0x7fc24f4360e0>, 'set_memory_limit': <function deprecation_warning.<locals>._wrapped at 0x7fc24f436200>, 'set_memory_request': <function deprecation_warning.<locals>._wrapped at 0x7fc24f436320>, 'set_readiness_probe': <function deprecation_warning.<locals>._wrapped at 0x7fc24f436440>, 'set_security_context': <function deprecation_warning.<locals>._wrapped at 0x7fc24f436560>, 'set_stdin': <function deprecation_warning.<locals>._wrapped at 0x7fc24f436680>, 'set_stdin_once': <function deprecation_warning.<locals>._wrapped at 0x7fc24f4367a0>, 'set_termination_message_path': <function deprecation_warning.<locals>._wrapped at 0x7fc24f4368c0>, 'set_termination_message_policy': <function deprecation_warning.<locals>._wrapped at 0x7fc24f4369e0>, 'set_tty': <function deprecation_warning.<locals>._wrapped at 0x7fc24f436b00>, 'input_artifact_paths': {}, 'artifact_arguments': {}, 'file_outputs': {'data': './output/out.json'}, 'output_artifact_paths': {}, '_metadata': None, 'execution_options': ExecutionOptionsSpec(retry_strategy=None, caching_strategy=CachingStrategySpec(max_cache_staleness=None), kubernetes_options=None), 'outputs': {'data': {{pipelineparam:op=one-pi-estimate 800007fc24f3a119;name=data}}}, 'output': {{pipelineparam:op=one-pi-estimate 800007fc24f3a119;name=data}}, 'pvolumes': {}, 'pvolume': None}}\n"
     ]
    }
   ],
   "source": [
    "pprint(_sample_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid argument. Must be of instance `V1EnvVar`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a90b3c43e584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mv1secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1Secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'V1Secret data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_sample_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_env_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;34m'Use `dsl.ContainerOp.container.%s` instead.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             (op_name, container_name), PendingDeprecationWarning)\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_decorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                 \u001b[0;31m# execute method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py\u001b[0m in \u001b[0;36madd_env_variable\u001b[0;34m(self, env_variable)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV1EnvVar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             raise ValueError(\n\u001b[0;32m--> 353\u001b[0;31m                 'invalid argument. Must be of instance `V1EnvVar`.')\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_and_append\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid argument. Must be of instance `V1EnvVar`."
     ]
    }
   ],
   "source": [
    "v1secret = V1Secret(data='V1Secret data')\n",
    "_sample_op.add_env_variable(v1secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid argument. Must be of instance `V1EnvVar`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cbca8a7c3e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msecret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1SecretKeySelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'secret_key'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"secret_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_sample_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_env_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;34m'Use `dsl.ContainerOp.container.%s` instead.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             (op_name, container_name), PendingDeprecationWarning)\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_decorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                 \u001b[0;31m# execute method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py\u001b[0m in \u001b[0;36madd_env_variable\u001b[0;34m(self, env_variable)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV1EnvVar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             raise ValueError(\n\u001b[0;32m--> 353\u001b[0;31m                 'invalid argument. Must be of instance `V1EnvVar`.')\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_and_append\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid argument. Must be of instance `V1EnvVar`."
     ]
    }
   ],
   "source": [
    "env_var = V1EnvVar(name='env_var_name', value='env_var_value')\n",
    "_sample_op.add_env_variable(env_var)\n",
    "\n",
    "secret = V1SecretKeySelector(key='secret_key', name=\"secret_name\")\n",
    "_sample_op.add_env_variable(secret)\n",
    "\n",
    "_sample_op.add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_op(jsons=None):\n",
    "    \"\"\"\n",
    "    Factory for \"average\" pipeline operation\n",
    "        \n",
    "    Operations created by this factory invoke the AVERAGE step by invoking\n",
    "    a docker container that includes ./average/average.sh.  average.sh accepts\n",
    "    results from one or more SAMPLE steps as JSON strings and returns output\n",
    "    as a JSON string\n",
    "\n",
    "    Generates an output file out.json with contents:\n",
    "        { \"pi\": estimate_of_pi, \"samples\": number_of_samples }\n",
    "    This result is passed back as _sample_op_result.output\n",
    "    \n",
    "    Args:\n",
    "        jsons (list): (optional) List of JSON strings of output from one or more \n",
    "                      \"sample\" steps\n",
    "\n",
    "    Returns:\n",
    "        JSON string of out.json\n",
    "    \"\"\"\n",
    "    if not jsons:\n",
    "        raise ValueError(\"Must specify one or more json string inputs\")\n",
    "    \n",
    "    # Assemble arguments passed to the script called by the container op\n",
    "    arguments = []\n",
    "    if jsons:\n",
    "        json_args = list(itertools.chain.from_iterable([(\"--json\", j) for j in jsons]))\n",
    "        arguments += json_args\n",
    "    \n",
    "    # And to the ContainerOp constructor\n",
    "    containerop_kwargs = dict(\n",
    "        name=AVERAGE_PIPELINE_OP_NAME,\n",
    "        image=f'{AVERAGE_IMAGE_PATH}',\n",
    "        arguments=arguments,\n",
    "        # Specify where to get output from\n",
    "        file_outputs={'data': \"./output/out.json\"},\n",
    "    )\n",
    "    \n",
    "    # Return the actual Container Op\n",
    "    return dsl.ContainerOp(**containerop_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the pipeline here as a python function wrapped in the @dsl.pipeline decorator.  This function, in our case compute_pi(), defines the logic for how all the steps within the pipeline chain together.  In our case, it tells kubeflow pipelines to run N **sample** operations in parallel, and run a single **average** operation that consumes output from all the **sample** operations.  \n",
    "\n",
    "This dependency of **average** on **sample**s is what lets kfp know the order in which to run things.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### You can change below this      ###\n",
    "### Create the pipeline            ###\n",
    "######################################\n",
    "@dsl.pipeline(\n",
    "    name=\"Estimate Pi\",\n",
    "    description='Estimate Pi using a Map-Reduce pattern'\n",
    ")\n",
    "def compute_pi():\n",
    "    \"\"\"Compute Pi\"\"\"\n",
    "\n",
    "    # Create arguments for each \"sample\" operation in the pipeline\n",
    "    # Each operation gets its own seed and a path in minio to store its output\n",
    "    # params is passed as a json string with just {'seed': value_of_seed}\n",
    "    sample_args = [{'params': json.dumps(param)} for (i, param) in enumerate(seeds())]\n",
    "\n",
    "    # Create a sample operation for each arg\n",
    "    sample_ops = [sample_op(**kwarg) for kwarg in sample_args]\n",
    "\n",
    "    # Define the average operation which consumes output from all the sample_ops\n",
    "    _average_op = average_op(\n",
    "        jsons=[s.output for s in sample_ops],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand here that while ```compute_pi``` describes the pipeline in python code, most of the computation is not done when we run the above block.  Calling ```sample_op``` does not do a **sample** operation, it creates a ContainerOp that tells kubeflow pipelines to run a **sample** operation when running the pipeline.  And when we do something like:\n",
    "```\n",
    "_average_op = average_op(\n",
    "    jsons=[s.output for s in sample_ops],\n",
    ")\n",
    "```\n",
    "\n",
    "```s.output``` is not the actual output of a **sample** operation, it is a placeholder that tells kubeflow pipelines \"when you get to this part in the pipeline, insert the output that you've previous computed for this **sample** operation here\".  This way you can pipe data from one pipeline step to the next without having to actually compute it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we translate our compute_pi function into a zipped yaml definition of the pipeline.  This zip file is how we tell kubeflow pipelines exactly what to run for your pipeline.  Download and take a look inside to get a better understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### DON'T EDIT:                             ###\n",
    "### Create the pipeline description for kfp ###\n",
    "###############################################\n",
    "from kfp import compiler\n",
    "experiment_yaml_zip = EXPERIMENT_NAME + '.zip'\n",
    "compiler.Compiler().compile(\n",
    "    compute_pi,\n",
    "    experiment_yaml_zip\n",
    ")\n",
    "print(f\"Exported pipeline definition to {experiment_yaml_zip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready to roll! Let's run this pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### DON'T EDIT:                 ###\n",
    "### Create the Experiment       ###\n",
    "###################################\n",
    "import kfp\n",
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### DON'T EDIT:                             ###\n",
    "### Run the pipeline                        ###\n",
    "###############################################\n",
    "import time\n",
    "run = client.run_pipeline(\n",
    "    exp.id,\n",
    "    EXPERIMENT_NAME + '-' + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    EXPERIMENT_NAME + '.zip',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the pipeline running, click the link above.  To access the JSON string returned by the Average step, click on that step in the pipeline and look in the output artifact as shown below.\n",
    "\n",
    "![pipeline with results](images/kf-pipeline_with_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this method of returning a result is likely not that useful for most problems.  See other other demos for saving results to minio or other locations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
